//! # mithril-dedup
//!
//! Data deduplication for ML training datasets.
//!
//! This crate provides efficient near-duplicate detection using MinHash and
//! Locality-Sensitive Hashing (LSH). It can process 100K+ documents per second.
//!
//! ## Example
//!
//! ```no_run
//! use mithril_dedup::{Deduplicator, DedupConfig};
//!
//! let config = DedupConfig::default();
//! let mut dedup = Deduplicator::new(config);
//!
//! // Add documents
//! let texts = vec![
//!     "The quick brown fox jumps over the lazy dog",
//!     "The quick brown fox jumps over the lazy dog",  // duplicate
//!     "A completely different document",
//! ];
//!
//! let result = dedup.deduplicate_texts(&texts);
//! println!("Found {} duplicates", result.stats.duplicate_count);
//! ```

pub mod cluster;
pub mod distributed;
pub mod hybrid;
pub mod io;
pub mod lsh;
pub mod lsh_bloom;
pub mod minhash;
pub mod semantic;
pub mod streaming;

use cluster::UnionFind;
use lsh::{DocId, LshIndex};
use minhash::{MinHashSignature, MinHasher, DEFAULT_NGRAM_SIZE, DEFAULT_NUM_PERMUTATIONS};
use rayon::prelude::*;
use std::collections::{HashMap, HashSet};
use thiserror::Error;

pub use distributed::{
    Coordinator, CoordinatorConfig, CoordinatorStats, Worker, WorkerConfig, WorkerInfo,
    WorkerResult,
};
pub use hybrid::{HybridConfig, HybridDeduplicator, HybridError, HybridStats};
/// Re-exports for convenience
pub use io::{
    read_jsonl, read_jsonl_with_original, read_parquet, write_jsonl, write_jsonl_lines,
    write_parquet, Document, InputFormat,
};
pub use lsh_bloom::{
    LshBloomBuilder, LshBloomIndex, LshBloomParams, MergeError, MergeStats, StreamingDeduplicator,
};
#[cfg(feature = "candle")]
pub use semantic::CandleBackend;
pub use semantic::{
    cosine_similarity, EmbeddingBackend, HnswConfig, HnswIndex, HnswMatch, MockBackend,
    SemanticConfig, SemanticDeduplicator, SemanticError, SemanticIndex, SemanticMatch,
};
pub use streaming::{
    deduplicate_jsonl, deduplicate_jsonl_with_threshold, deduplicate_parquet, CumulativeStats,
    IncrementalIndex, StreamingConfig, StreamingProcessor, StreamingStats,
};

/// Configuration for the deduplicator.
#[derive(Debug, Clone)]
pub struct DedupConfig {
    /// Similarity threshold (0.0-1.0). Documents with similarity >= threshold are duplicates.
    pub threshold: f64,
    /// Number of MinHash permutations. More = more accurate, but slower.
    pub num_permutations: usize,
    /// N-gram size for shingling. Larger = more specific matching.
    pub ngram_size: usize,
    /// Whether to verify candidates with exact MinHash similarity.
    pub verify_candidates: bool,
}

impl Default for DedupConfig {
    fn default() -> Self {
        Self {
            threshold: 0.85,
            num_permutations: DEFAULT_NUM_PERMUTATIONS,
            ngram_size: DEFAULT_NGRAM_SIZE,
            verify_candidates: true,
        }
    }
}

impl DedupConfig {
    /// Create a new config with the specified threshold.
    #[must_use]
    pub fn with_threshold(threshold: f64) -> Self {
        Self {
            threshold,
            ..Default::default()
        }
    }
}

/// Statistics about the deduplication process.
#[derive(Debug, Clone, Default)]
pub struct DedupStats {
    /// Total number of documents processed.
    pub total_documents: usize,
    /// Number of unique documents (not duplicates).
    pub unique_documents: usize,
    /// Number of duplicate documents found.
    pub duplicate_count: usize,
    /// Ratio of duplicates to total documents.
    pub duplicate_ratio: f64,
    /// Number of duplicate clusters found.
    pub cluster_count: usize,
    /// Number of candidate pairs generated by LSH.
    pub candidate_pairs: usize,
    /// Number of pairs that passed verification.
    pub verified_pairs: usize,
    /// Processing time in seconds.
    pub processing_time_secs: f64,
}

/// Result of deduplication.
#[derive(Debug, Clone)]
pub struct DedupResult {
    /// Indices of documents to keep (one per cluster).
    pub keep_indices: Vec<usize>,
    /// Indices of duplicate documents to remove.
    pub remove_indices: Vec<usize>,
    /// Duplicate clusters (representative_index -> duplicate_indices).
    pub clusters: HashMap<usize, Vec<usize>>,
    /// Deduplication statistics.
    pub stats: DedupStats,
}

/// Error types for deduplication.
#[derive(Error, Debug)]
pub enum DedupError {
    #[error("Invalid configuration: {0}")]
    InvalidConfig(String),

    #[error("IO error: {0}")]
    Io(#[from] io::IoError),

    #[error("Processing error: {0}")]
    Processing(String),
}

/// Result type for deduplication operations.
pub type Result<T> = std::result::Result<T, DedupError>;

/// Main deduplication engine.
///
/// Uses MinHash signatures and LSH bucketing to efficiently find near-duplicate documents.
pub struct Deduplicator {
    config: DedupConfig,
    hasher: MinHasher,
}

impl Deduplicator {
    /// Create a new deduplicator with the given configuration.
    #[must_use]
    pub fn new(config: DedupConfig) -> Self {
        let hasher = MinHasher::new(config.num_permutations).with_ngram_size(config.ngram_size);

        Self { config, hasher }
    }

    /// Get the configuration.
    #[must_use]
    pub fn config(&self) -> &DedupConfig {
        &self.config
    }

    /// Deduplicate a slice of text strings.
    ///
    /// Returns indices of documents to keep and remove.
    #[must_use]
    pub fn deduplicate_texts(&self, texts: &[&str]) -> DedupResult {
        let start = std::time::Instant::now();

        // Generate signatures in parallel
        let signatures: Vec<MinHashSignature> = texts
            .par_iter()
            .map(|text| self.hasher.signature_from_text(text))
            .collect();

        // Build LSH index
        let mut lsh = LshIndex::with_threshold(self.config.num_permutations, self.config.threshold);
        for (i, sig) in signatures.iter().enumerate() {
            lsh.insert(i as DocId, sig);
        }

        // Get candidate pairs
        let candidates = lsh.candidates_vec();
        let candidate_count = candidates.len();

        // Verify candidates and build union-find
        let mut uf = UnionFind::new(texts.len());
        let mut verified_count = 0;

        for (id1, id2) in candidates {
            let i = id1 as usize;
            let j = id2 as usize;

            let is_duplicate = if self.config.verify_candidates {
                let sim = MinHasher::similarity(&signatures[i], &signatures[j]);
                sim >= self.config.threshold
            } else {
                true
            };

            if is_duplicate {
                uf.union(i, j);
                verified_count += 1;
            }
        }

        // Extract clusters
        let clusters = uf.duplicate_clusters();

        // Determine which documents to keep and remove
        let mut keep_indices = Vec::new();
        let mut remove_indices = Vec::new();
        let mut cluster_map: HashMap<usize, Vec<usize>> = HashMap::new();

        // First, handle clustered documents
        let mut in_cluster = HashSet::new();
        for members in clusters.values() {
            in_cluster.extend(members.iter().copied());
            // Keep the representative (first member when sorted)
            let mut sorted_members = members.clone();
            sorted_members.sort();
            let representative = sorted_members[0];
            keep_indices.push(representative);

            // Mark others as duplicates
            let duplicates: Vec<usize> = sorted_members[1..].to_vec();
            remove_indices.extend(duplicates.iter().copied());
            cluster_map.insert(representative, duplicates);
        }

        // Add non-clustered documents to keep list
        for i in 0..texts.len() {
            if !in_cluster.contains(&i) {
                keep_indices.push(i);
            }
        }

        keep_indices.sort();
        remove_indices.sort();

        let elapsed = start.elapsed().as_secs_f64();

        DedupResult {
            keep_indices,
            remove_indices: remove_indices.clone(),
            clusters: cluster_map,
            stats: DedupStats {
                total_documents: texts.len(),
                unique_documents: texts.len() - remove_indices.len(),
                duplicate_count: remove_indices.len(),
                duplicate_ratio: remove_indices.len() as f64 / texts.len() as f64,
                cluster_count: clusters.len(),
                candidate_pairs: candidate_count,
                verified_pairs: verified_count,
                processing_time_secs: elapsed,
            },
        }
    }

    /// Deduplicate documents from the io module.
    #[must_use]
    pub fn deduplicate_documents(&self, docs: &[Document]) -> DedupResult {
        let texts: Vec<&str> = docs.iter().map(|d| d.text.as_str()).collect();
        self.deduplicate_texts(&texts)
    }

    /// Compute the MinHash signature for a text.
    #[must_use]
    pub fn signature(&self, text: &str) -> MinHashSignature {
        self.hasher.signature_from_text(text)
    }

    /// Compute similarity between two texts.
    #[must_use]
    pub fn similarity(&self, text1: &str, text2: &str) -> f64 {
        let sig1 = self.hasher.signature_from_text(text1);
        let sig2 = self.hasher.signature_from_text(text2);
        MinHasher::similarity(&sig1, &sig2)
    }
}

impl Default for Deduplicator {
    fn default() -> Self {
        Self::new(DedupConfig::default())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_dedup_identical_documents() {
        let dedup = Deduplicator::default();
        let texts = vec![
            "The quick brown fox jumps over the lazy dog",
            "The quick brown fox jumps over the lazy dog",
            "A completely different document with unique content",
        ];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 3);
        assert_eq!(result.stats.duplicate_count, 1);
        assert_eq!(result.keep_indices.len(), 2);
        assert_eq!(result.remove_indices.len(), 1);
    }

    #[test]
    fn test_dedup_no_duplicates() {
        let dedup = Deduplicator::default();
        let texts = vec![
            "Document one about topic A",
            "Document two about topic B",
            "Document three about topic C",
        ];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 3);
        assert_eq!(result.stats.duplicate_count, 0);
        assert_eq!(result.keep_indices.len(), 3);
        assert!(result.remove_indices.is_empty());
    }

    #[test]
    fn test_dedup_all_duplicates() {
        let dedup = Deduplicator::default();
        let text = "The same document repeated multiple times for testing";
        let texts = vec![text, text, text, text];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 4);
        assert_eq!(result.stats.duplicate_count, 3);
        assert_eq!(result.keep_indices.len(), 1);
        assert_eq!(result.remove_indices.len(), 3);
    }

    #[test]
    fn test_dedup_with_threshold() {
        let config = DedupConfig::with_threshold(0.9);
        let dedup = Deduplicator::new(config);

        let texts = vec![
            "The quick brown fox jumps over the lazy dog",
            "The quick brown fox leaps over the lazy dog", // slightly different
        ];

        let result = dedup.deduplicate_texts(&texts);

        // With high threshold, these might not be detected as duplicates
        // depending on the similarity
        assert!(result.stats.total_documents == 2);
    }

    #[test]
    fn test_similarity() {
        let dedup = Deduplicator::default();

        let text1 = "The quick brown fox jumps over the lazy dog";
        let text2 = "The quick brown fox jumps over the lazy dog";

        let sim = dedup.similarity(text1, text2);
        assert!((sim - 1.0).abs() < f64::EPSILON);
    }

    #[test]
    fn test_dedup_clusters() {
        let dedup = Deduplicator::default();

        // Create two clusters of duplicates
        let texts = vec![
            "Cluster one document A about topic X",
            "Cluster one document A about topic X", // dup of 0
            "Cluster two document B about topic Y",
            "Cluster two document B about topic Y", // dup of 2
            "Unique document C about topic Z",
        ];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 5);
        assert_eq!(result.stats.duplicate_count, 2);
        assert_eq!(result.stats.cluster_count, 2);
    }

    #[test]
    fn test_dedup_empty_input() {
        let dedup = Deduplicator::default();
        let texts: Vec<&str> = vec![];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 0);
        assert_eq!(result.stats.duplicate_count, 0);
        assert!(result.keep_indices.is_empty());
    }

    #[test]
    fn test_dedup_single_document() {
        let dedup = Deduplicator::default();
        let texts = vec!["Single document"];

        let result = dedup.deduplicate_texts(&texts);

        assert_eq!(result.stats.total_documents, 1);
        assert_eq!(result.stats.duplicate_count, 0);
        assert_eq!(result.keep_indices.len(), 1);
    }

    #[test]
    fn test_dedup_documents() {
        let dedup = Deduplicator::default();

        let docs = vec![
            Document::new(1, "Document one text".to_string()),
            Document::new(2, "Document one text".to_string()), // duplicate
            Document::new(3, "Document three text".to_string()),
        ];

        let result = dedup.deduplicate_documents(&docs);

        assert_eq!(result.stats.total_documents, 3);
        assert_eq!(result.stats.duplicate_count, 1);
    }
}
