#!/usr/bin/env python3
"""Generate test fixtures for mithril testing."""
import json
import os
import random
import struct
from pathlib import Path


def generate_checkpoint(path: str, size_mb: int = 10) -> None:
    """Generate synthetic PyTorch-like checkpoint (raw bf16 data).

    Note: This generates raw bf16 bytes, not an actual PyTorch checkpoint.
    For actual .pt files, use torch.save() when torch is available.
    """
    num_values = size_mb * 1024 * 1024 // 2  # bf16 = 2 bytes

    # Generate random bf16 values (just random 2-byte values)
    data = bytearray()
    for _ in range(num_values):
        # Random bf16: sign bit + 8-bit exponent + 7-bit mantissa
        value = random.randint(0, 0xFFFF)
        data.extend(struct.pack('<H', value))

    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'wb') as f:
        f.write(data)

    actual_size_mb = len(data) / (1024 * 1024)
    print(f"Created {path} ({actual_size_mb:.1f}MB, {num_values} bf16 values)")


def generate_dedup_dataset(path: str, num_docs: int = 1000, dup_ratio: float = 0.3) -> None:
    """Generate JSONL with known duplicates for testing."""
    unique_count = int(num_docs * (1 - dup_ratio))
    docs = []

    # Create unique docs with varied content
    templates = [
        "This is document {} containing information about topic {}.",
        "Article {} discusses the subject of {} in detail.",
        "Report number {} covers the analysis of {}.",
        "Study {} examines the relationship between {} and related factors.",
    ]

    topics = [
        "machine learning", "data processing", "neural networks",
        "optimization", "training", "inference", "models", "datasets"
    ]

    for i in range(unique_count):
        template = random.choice(templates)
        topic = random.choice(topics)
        text = template.format(i, topic)
        # Add some random suffix to make each doc unique
        text += f" Random suffix: {random.random():.10f}"
        docs.append({"id": i, "text": text})

    # Create duplicates (exact copies)
    for i in range(num_docs - unique_count):
        original = docs[i % unique_count]
        docs.append({
            "id": unique_count + i,
            "text": original["text"],
            "duplicate_of": original["id"]  # For verification
        })

    random.shuffle(docs)

    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w') as f:
        for doc in docs:
            f.write(json.dumps(doc) + '\n')

    print(f"Created {path} ({num_docs} docs, {dup_ratio:.0%} duplicates)")


def main():
    # Set random seed for reproducibility
    random.seed(42)

    # Get project root (scripts is one level down)
    project_root = Path(__file__).parent.parent
    fixtures_dir = project_root / "fixtures"

    # Generate checkpoint fixture (10MB bf16 data)
    generate_checkpoint(
        str(fixtures_dir / "checkpoints" / "small_model.bin"),
        size_mb=10
    )

    # Generate dedup fixture (1000 docs, 30% duplicates)
    generate_dedup_dataset(
        str(fixtures_dir / "datasets" / "duplicates.jsonl"),
        num_docs=1000,
        dup_ratio=0.3
    )

    # Create README
    readme_path = fixtures_dir / "README.md"
    readme_path.write_text("""# Test Fixtures

Generated by `scripts/generate_fixtures.py`.

## Files

- `checkpoints/small_model.bin` - 10MB of random bf16 data for checkpoint compression testing
- `datasets/duplicates.jsonl` - 1000 documents with 30% duplicates for dedup testing

## Regenerate

```bash
python scripts/generate_fixtures.py
```

## Verification

The duplicates.jsonl file includes a `duplicate_of` field for exact duplicates,
making it easy to verify dedup accuracy.
""")
    print(f"Created {readme_path}")


if __name__ == "__main__":
    main()
